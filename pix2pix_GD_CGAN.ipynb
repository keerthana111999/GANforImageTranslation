{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import clip\n",
    "import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from patchify import patchify, unpatchify\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(input_images,output1_images,output2_images):\n",
    "    input_list = []\n",
    "    output1_list = []\n",
    "    output2_list = []\n",
    "    for i,j,k in zip(input_images,output1_images,output2_images):\n",
    "        image = Image.open(i)\n",
    "        arr_image = np.asarray(image) #image to array\n",
    "        input_list.append(arr_image);\n",
    "\n",
    "        image = Image.open(j)\n",
    "        arr_image = np.asarray(image) #image to array\n",
    "        output1_list.append(arr_image);\n",
    "\n",
    "        image = Image.open(k)\n",
    "        arr_image = np.asarray(image) #image to array\n",
    "        output2_list.append(arr_image);\n",
    "    return input_list, output1_list,output2_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that uses patchify to slice the images into given window size\n",
    "#parameters 1.list of images(as an array of pixels) 2.common prefix for the patched images 3. name of the folder to place the patches\n",
    "def image_split(large_image_stack,window_size,step_size):\n",
    "    all_img_patches = []\n",
    "    for k in range(len(large_image_stack)):\n",
    "        large_image = large_image_stack[k]\n",
    "        patches_img = patchify(large_image, (window_size, window_size), step=step_size)  # no overlap\n",
    "        for i in range(patches_img.shape[0]):\n",
    "            for j in range(patches_img.shape[1]):\n",
    "                single_patch_img = patches_img[i,j,:,:]\n",
    "                all_img_patches.append(single_patch_img)\n",
    "    images = np.array(all_img_patches)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7 7\n",
      "Performing Image Splitting......\n",
      "\n",
      "Done:)\n"
     ]
    }
   ],
   "source": [
    "# Running Just Patchify+ImageAugment to augment the image of size 256 x 256 and step = 51\n",
    "#folder name = /home/lakram9u/py_scripts/D2DB/Generated_Data/Patchify_ImgGen_256x256\n",
    "\n",
    "\n",
    "input_images = glob.glob(\"INSERT YOUR PATH\")\n",
    "output1_images = glob.glob(\"INSERT YOUR PATH\")\n",
    "output2_images = glob.glob(\"INSERT YOUR PATH\")\n",
    "\n",
    "#converting the images to arrays\n",
    "input_list, output1_list,output2_list = pre_processing(input_images,output1_images,output2_images)\n",
    "\n",
    "print(len(input_list),len(output1_list),len(output2_list))\n",
    "print(\"Performing Image Splitting......\\n\")\n",
    "\n",
    "#Patchify\n",
    "window_size = 256\n",
    "step_size = 14\n",
    "\n",
    "input_split_images = image_split(input_list,\n",
    "                                window_size,\n",
    "                                step_size)\n",
    "\n",
    "output1_split_images = image_split(output1_list,\n",
    "                                  window_size,\n",
    "                                  step_size)\n",
    "\n",
    "output2_split_images = image_split(output2_list,                                   \n",
    "                                   window_size,\n",
    "                                   step_size)\n",
    "print(\"Done:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding 3rd dimension for channel\n",
    "input_list = []\n",
    "output1_list = []\n",
    "output2_list = []\n",
    "for i in range(input_split_images.shape[0]):\n",
    "    arr_image = input_split_images[i]/255.0\n",
    "    input_array = arr_image.reshape(arr_image.shape + (1,))\n",
    "    input_list.append(input_array);\n",
    "    \n",
    "    arr_image = output1_split_images[i]/255.0\n",
    "    input_array = arr_image.reshape(arr_image.shape + (1,))\n",
    "    output1_list.append(input_array);\n",
    "    \n",
    "    arr_image = output2_split_images[i]/255.0\n",
    "    input_array = arr_image.reshape(arr_image.shape + (1,))\n",
    "    output2_list.append(input_array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = np.array(input_list)\n",
    "output1_list = np.array(output1_list)\n",
    "output2_list = np.array(output2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2527, 256, 256, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output1_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "inp, inp_test, out1, out1_test, out2, out2_test = train_test_split(input_list, output1_list, output2_list, test_size=0.2, random_state=42)\n",
    "inp_train,inp_val,out1_train,out1_val,out2_train,out2_val = train_test_split(inp, out1, out2, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1616, 256, 256, 1) (405, 256, 256, 1) (506, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "print(inp_train.shape,inp_val.shape,inp_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training set size\n",
    "BUFFER_SIZE = 1616\n",
    "# The batch size of 1 produced better results for the U-Net in the original pix2pix experiment\n",
    "BATCH_SIZE = 1\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 1\n",
    "LAMBDA = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n",
    "\n",
    "#Encoder\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
    "    downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
    "    downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
    "    downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
    "    downsample(512, 4),  # (batch_size, 8, 8, 512)\n",
    "    downsample(512, 4),  # (batch_size, 4, 4, 512)\n",
    "    downsample(512, 4),  # (batch_size, 2, 2, 512)\n",
    "    downsample(512, 4),  # (batch_size, 1, 1, 512)\n",
    "  ]\n",
    "#Decoder\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n",
    "    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n",
    "    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n",
    "    upsample(256, 4),  # (batch_size, 32, 32, 512)\n",
    "    upsample(128, 4),  # (batch_size, 64, 64, 256)\n",
    "    upsample(64, 4),  # (batch_size, 128, 128, 128)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "#tf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "#tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np \n",
    "#inp = np.random.rand(1,256,256,1)\n",
    "#prediction = generator(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/\"\n",
    "\n",
    "summary_writer = tf.summary.create_file_writer(log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_image, target, step):\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    gen_output = generator(input_image, training=True)\n",
    "\n",
    "    disc_real_output = discriminator([input_image, target], training=True)\n",
    "    disc_generated_output = discriminator([input_image, gen_output], training=True)\n",
    "    \n",
    "    gen_output = tf.cast(gen_output, tf.float64)\n",
    "    disc_generated_output = tf.cast(disc_generated_output,tf.float64)\n",
    "    disc_real_output = tf.cast(disc_real_output,tf.float64)\n",
    "    \n",
    "    gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output[0], gen_output[0], target[0])\n",
    "    disc_loss = discriminator_loss(disc_real_output[0], disc_generated_output[0])\n",
    "\n",
    "  generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          generator.trainable_variables)\n",
    "  discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "\n",
    "  generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          generator.trainable_variables))\n",
    "  discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))\n",
    "\n",
    "  with summary_writer.as_default():\n",
    "    tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1616)\n",
    "    tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1616)\n",
    "    tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1616)\n",
    "    tf.summary.scalar('disc_loss', disc_loss, step=step//1616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(inp_ds, out_ds, steps):\n",
    "  \n",
    "  start = time.time()\n",
    "  for step, (input_image, target) in zip(range(steps), zip(inp_ds, out_ds)):\n",
    "    if (step) % 1616 == 0:\n",
    "\n",
    "      if step != 0:\n",
    "        print(f'Time taken for epoch {step/1616:.2f}: {time.time()-start:.2f} sec\\n')\n",
    "    input_image = tf.reshape(input_image, (1, 256, 256, 1))\n",
    "    target = tf.reshape(target,(1,256,256,1))\n",
    "    train_step(input_image, target, step)\n",
    "\n",
    "    # Training step\n",
    "    if (step+1) % 10 == 0:\n",
    "      print('.', end='', flush=True)\n",
    "\n",
    "\n",
    "    # Save (checkpoint) the model every epoch\n",
    "    if (step) % 1616 == 0:\n",
    "      checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "  end = time.time()\n",
    "  time_taken = end - start\n",
    "  print(f\"Time taken: {time_taken:.6f} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: kill: (3107615) - No such process\r\n"
     ]
    }
   ],
   "source": [
    "!kill 3107615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3107615), started 0:09:11 ago. (Use '!kill 3107615' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7176e9b53db6a6cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7176e9b53db6a6cd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {log_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80800, 256, 256, 1) (80800, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "inp_train = np.repeat(inp_train, 50, axis=0)\n",
    "out1_train = np.repeat(out1_train,50,axis=0)\n",
    "#fit(inp_train, out1_train, steps=80800)\n",
    "print(inp_train.shape,out1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................Time taken for epoch 1.00: 10978.19 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 2.00: 17665.20 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 3.00: 26568.93 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 4.00: 32828.56 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 5.00: 41728.15 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 6.00: 50496.08 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 7.00: 57848.32 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 8.00: 63433.13 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 9.00: 68991.88 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 10.00: 74580.85 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 11.00: 80118.65 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 12.00: 85708.88 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 13.00: 91342.57 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 14.00: 97112.99 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 15.00: 102720.69 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 16.00: 108298.95 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 17.00: 113894.91 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 18.00: 119445.31 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 19.00: 125031.63 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 20.00: 130609.32 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 21.00: 136200.46 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 22.00: 141789.88 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 23.00: 147400.56 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 24.00: 152959.70 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 25.00: 154562.91 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 26.00: 155971.14 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 27.00: 157379.89 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 28.00: 158784.90 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 29.00: 160192.65 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 30.00: 163101.76 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 31.00: 168662.36 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 32.00: 174253.20 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 33.00: 179826.51 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 34.00: 185422.20 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 35.00: 191006.88 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 36.00: 196645.57 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 37.00: 202254.08 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 38.00: 207869.93 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 39.00: 213441.46 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 40.00: 219037.54 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 41.00: 224630.06 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 42.00: 229393.05 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 43.00: 237649.31 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 44.00: 246937.37 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 45.00: 256256.44 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 46.00: 265609.01 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 47.00: 274965.91 sec\n",
      "\n",
      ".................................................................................................................................................................Time taken for epoch 48.00: 284378.80 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken for epoch 49.00: 293774.94 sec\n",
      "\n",
      "..................................................................................................................................................................Time taken: 303172.896043 seconds\n"
     ]
    }
   ],
   "source": [
    "fit(inp_train, out1_train, steps=80800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_pred = generator(inp_test,training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_pred = out1_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 256, 256, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
